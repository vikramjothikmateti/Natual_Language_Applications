{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7871991",
   "metadata": {},
   "source": [
    "\n",
    "# Sentiment Analysis Using Na√Øve Bayes Text Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c0f4c",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Import the necessary libraries\n",
    "2. Load and clean the data\n",
    "Handle missing values.\n",
    "Convert labels from neg and pos to 0 and 1.\n",
    "3. Preprocessing the text\n",
    "Remove punctuation and symbols using regular expressions.\n",
    "Lemmatization, stop word removal, and handling logical negations.\n",
    "4. Split the data into training and testing sets (80% train, 20% test)\n",
    "5. Train a Naive Bayes classifier\n",
    "Compute prior and likelihood probabilities.\n",
    "Implement Laplace smoothing.\n",
    "6. Evaluate the model\n",
    "Test the model on the 20% test set.\n",
    "Generate confusion matrix and calculate precision, recall, and F1 score.\n",
    "7. Run the experiments with different preprocessing configurations:\n",
    "Without lemmatization, stop word removal, or negation handling.\n",
    "With lemmatization.\n",
    "With lemmatization and stop word removal.\n",
    "With lemmatization, stop word removal, and negation handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56c63996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score,accuracy_score\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load the spacy English model for lemmatization and negation handling\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de0ecab",
   "metadata": {},
   "source": [
    "Load and clean the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ba1c41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1965, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data from the .tsv file\n",
    "data = pd.read_table('moviereviews.tsv')\n",
    "\n",
    "# Drop missing or null values in the reviews\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Convert the labels from 'neg' to 0 and 'pos' to 1\n",
    "data['label'] = data['label'].map({'neg': 0, 'pos': 1})\n",
    "\n",
    "# Quick check on the data\n",
    "data.head()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b63e63",
   "metadata": {},
   "source": [
    "Text Preprocessing Function:\n",
    "\n",
    "1. Punctuation Removal: Removes punctuation and special symbols using a regex pattern.\n",
    "2. Convert to spaCy Document: Converts the text to a spaCy document object for NLP processing.\n",
    "3. Token Processing: Iterates through tokens, handling logical negations (e.g., prefixing with \"NOT_\") and lemmatizing (getting base form) if enabled.\n",
    "4. Stop Word Removal: Removes stop words (common words like \"the\", \"and\") if enabled.\n",
    "5. Output: Returns the processed text as a space-separated string of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdf7a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, lemmatize_words=True, remove_stop_words=True, handle_logical_negation=True):\n",
    "    # Remove punctuation and symbols using regex\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Convert to a spaCy document object\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    processed_tokens = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Handle logical negations if enabled\n",
    "        if handle_logical_negation and token.dep_ == 'neg':\n",
    "            processed_tokens.append('NOT_' + token.head.lemma_)\n",
    "        elif lemmatize_words:\n",
    "            processed_tokens.append(token.lemma_)\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "    \n",
    "    # Remove stop words if the flag is enabled\n",
    "    if remove_stop_words:\n",
    "        processed_tokens = [token for token in processed_tokens if not nlp.vocab[token].is_stop]\n",
    "    \n",
    "    return ' '.join(processed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f0996",
   "metadata": {},
   "source": [
    "Train-test Split:\n",
    "\n",
    "1. The function `prepare_data` takes in a dataset and preprocessing arguments for text data.\n",
    "2. It preprocesses the 'review' column of the dataset using `preprocess_text` and stores the result in a new column `processed_review`.\n",
    "3. The data is then split into training (80%) and testing (20%) sets.\n",
    "4. A `CountVectorizer` is used to transform the text data into feature vectors for both training and testing sets.\n",
    "5. The function returns the vectorized training and testing sets along with their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2e51d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function for training and testing sets\n",
    "def prepare_data(data, preprocess_args):\n",
    "    # Preprocess reviews based on the provided arguments\n",
    "    data['processed_review'] = data['review'].apply(lambda x: preprocess_text(x, **preprocess_args))\n",
    "    \n",
    "    # Split data into training (80%) and testing (20%)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data['processed_review'], data['label'], test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Use CountVectorizer to transform text data into feature vectors\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_train_vect = vectorizer.fit_transform(X_train)\n",
    "    X_test_vect = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_vect, X_test_vect, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab830cb",
   "metadata": {},
   "source": [
    "Training Naive Bayes Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c456caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_bayes(X_train_vect, y_train):\n",
    "    # Initialize the Naive Bayes classifier\n",
    "    model = MultinomialNB()\n",
    "    # Train the model\n",
    "    model.fit(X_train_vect, y_train)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b14c0",
   "metadata": {},
   "source": [
    "Evaluate the Model:\n",
    "This function, `evaluate_model`, evaluates a machine learning model's performance:\n",
    "\n",
    "1. It takes a trained model, test data (`X_test_vect`), and the true labels (`y_test`).\n",
    "2. The model predicts labels (`y_pred`) for the test data.\n",
    "3. A confusion matrix is calculated to summarize prediction performance.\n",
    "4. It computes precision, recall, and F1 score to assess classification quality.\n",
    "5. It returns the confusion matrix, precision, recall,accuracy and F1 score as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d03c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test_vect, y_test):\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test_vect)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate precision, recall, and f1 score\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return conf_matrix, precision, recall, f1, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fac914",
   "metadata": {},
   "source": [
    "Running the Experiments:\n",
    "\n",
    "This code defines different preprocessing configurations for text data, with options like lemmatization, stop word removal, and logical negation handling. It then iterates over each configuration, applying them to the data. In each iteration, the data is preprocessed and split into training and test sets. The model is trained using Naive Bayes on the preprocessed training data. Finally, the model is evaluated by computing metrics such as confusion matrix, precision, recall, and F1 score, which are printed for each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acc9bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1: {'lemmatize_words': False, 'remove_stop_words': False, 'handle_logical_negation': False}\n",
      "Confusion Matrix:\n",
      "[[169  33]\n",
      " [ 50 141]]\n",
      "Precision: 0.8103448275862069, Recall: 0.7382198952879581, F1 Score: 0.7726027397260274, Accuracy: 0.7888040712468194\n",
      "\n",
      "Scenario 2: {'lemmatize_words': True, 'remove_stop_words': False, 'handle_logical_negation': False}\n",
      "Confusion Matrix:\n",
      "[[170  32]\n",
      " [ 50 141]]\n",
      "Precision: 0.815028901734104, Recall: 0.7382198952879581, F1 Score: 0.7747252747252747, Accuracy: 0.7913486005089059\n",
      "\n",
      "Scenario 3: {'lemmatize_words': True, 'remove_stop_words': True, 'handle_logical_negation': False}\n",
      "Confusion Matrix:\n",
      "[[166  36]\n",
      " [ 46 145]]\n",
      "Precision: 0.8011049723756906, Recall: 0.7591623036649214, F1 Score: 0.7795698924731181, Accuracy: 0.7913486005089059\n",
      "\n",
      "Scenario 4: {'lemmatize_words': True, 'remove_stop_words': True, 'handle_logical_negation': True}\n",
      "Confusion Matrix:\n",
      "[[165  37]\n",
      " [ 46 145]]\n",
      "Precision: 0.7967032967032966, Recall: 0.7591623036649214, F1 Score: 0.7774798927613941, Accuracy: 0.7888040712468194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define different preprocessing configurations\n",
    "scenarios = [\n",
    "    {\"lemmatize_words\": False, \"remove_stop_words\": False, \"handle_logical_negation\": False},\n",
    "    {\"lemmatize_words\": True, \"remove_stop_words\": False, \"handle_logical_negation\": False},\n",
    "    {\"lemmatize_words\": True, \"remove_stop_words\": True, \"handle_logical_negation\": False},\n",
    "    {\"lemmatize_words\": True, \"remove_stop_words\": True, \"handle_logical_negation\": True}\n",
    "]\n",
    "\n",
    "# Loop through each scenario and evaluate the model\n",
    "for i, preprocess_args in enumerate(scenarios, 1):\n",
    "    print(f\"Scenario {i}: {preprocess_args}\")\n",
    "    \n",
    "    # Preprocess data and split into training and test sets\n",
    "    X_train_vect, X_test_vect, y_train, y_test = prepare_data(data, preprocess_args)\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_naive_bayes(X_train_vect, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    conf_matrix, precision, recall, f1,accuracy = evaluate_model(model, X_test_vect, y_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}, Accuracy: {accuracy}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c7287",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
